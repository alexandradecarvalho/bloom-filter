{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLOOM FILTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Bloom filters are a data structure that allow rapidly checking whether a certain element belongs to a known set.\n",
    "With a big set of data, a **linear search** would be reeeaaaaly slow! So, one way of solving this problem is, given that we only want a \"yes/no\" answer, only storing 1s and 0s representing the element's existance in the set, and not the whole data.\n",
    "\n",
    "While this is super efficient, the downfall of this structure is its probabilistic nature, so it can contain some false positives. A **false positive** is when given a value, the algorithm returns 1 by mistake, when it should return 0. In this case, the false positive rate increases as the percentage of elements == 1 increases. The false positive rate can be calculated with:\n",
    "\n",
    "$ g(z) = \\frac{1}{(1+e^{-z})}$\n",
    "\n",
    " On the other hand, bloom filters never generate **false negatives**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we implemented this code is that so over a given value are calculated *nhash* hash functions. Each of them will return a given bloom filter position (values between [0,m-1]), which we will set to 1. Then, when searching if an element is in the set, it only returns ***True*** if all *nhash* positions are set to 1. Using multiple indices combined helps lowering the false positive cases. It's important that the hash functions used are independent among them, because if they weren't, it would be the same as just using one hash function. In our implementation, we will use the fast and robust set of hash functions Murmurhash3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import mmh3\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, nitems, false_positive_prob):\n",
    "        self.size = int(-(nitems * numpy.log(false_positive_prob))/(numpy.log(2)**2)) # size of the bloom filter\n",
    "        self.nhash = int((self.size/nitems) * numpy.log(2)) # number of hash functions\n",
    "\n",
    "        self.bloomf = numpy.zeros(self.size) # initializing the bloom filter as an array of size self.size with everything at 0\n",
    " \n",
    "    def add(self, item):\n",
    "        for i in range(self.nhash):\n",
    "            h = mmh3.hash(item, i) % self.size #i = seed\n",
    "            self.bloomf[h] = 1\n",
    " \n",
    "    def check(self, item):\n",
    "        for i in range(self.nhash):\n",
    "\n",
    "            h = mmh3.hash(item, i) % self.size\n",
    "\n",
    "            if self.bloomf[h] == 0:\n",
    "                return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chunks = pd.read_csv(\"dataset/library-collection-inventory.csv\", chunksize=1000000)\n",
    "our_set = pd.concat(chunks)\n",
    "print(type(our_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (/home/alexa/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:301742)",
      "at S.execute (/home/alexa/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:300732)",
      "at S.start (/home/alexa/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/alexa/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (/home/alexa/.vscode/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n = len(our_set) #no of items to add\n",
    "p = 0.05 #false positive probability\n",
    "\n",
    "titles_bf = BloomFilter(n,p)\n",
    "authors_bf = BloomFilter(n,p)\n",
    "subjects_bf = BloomFilter(n,p)\n",
    "\n",
    "for item in our_set:\n",
    "\t# TODO : clean these fields\n",
    "\ttitles_bf.add(item[1]).split(\"/\")[0]\n",
    "\tauthors_bf.add(item[2])\n",
    "\tsubjects_bf.add(item[6])\n",
    "\n",
    "while True:\n",
    "\t# TODO : add menu \n",
    "\tinpt = input(\"Search: \")\n",
    "\tif not inpt:\n",
    "\t\tbreak\n",
    "\n",
    "\tif titles_bf.check(inpt):\n",
    "\t\tprint(\"The introduced query is in the dataset\")\n",
    "\telse:\n",
    "\t\tprint(\"We're sorry, but we didn't find a result.\")\n",
    "\n",
    "# TODO : fix error"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
